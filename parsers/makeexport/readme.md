Data format and generation process for Book Review Digest data
===============================================================

This release contains four tab-separated tables that report data about book reviews excerpted in the Book Review Digest. It also contains the code that immediately produced these files, and the present readme file.

what's in the tables
--------------------

The tables contain factual bibliographic data about the books reviewed, and the magazines reviewing them. They also contain bags-of-words used in each review excerpt, sorted alphabetically--but not sequential text. They contain the 'average' sentiment value of reviews for a book, but don't report the actual review sentiment (expressed as plusses or minuses) at a per-review level. Finally, they don't include subject headings or index headings from the BRD.

the code used
--------------

make_export.py is the script that immediately produced these files; it's included here and you can inspect it to see what was selected or omitted.

If you're interested in seeing the script in context, you can find it in /home/dcuser/reviews/parsers/makeexport. It draws on two sources

1) The parsed review metadata itself, which is in /media/secure_volume/brd/output. These files were created by scripts in /home/dcuser/reviews/parsers/brd1930s.

2) An attempt to align the review metadata with the fiction section of the index. These files are in /media/secure_volume/brd/paired, and are generated by /home/dcuser/reviews/parsers/makeexport/pair_index_with_reviews.py.


Columns in the data
-------------------

bookauthor: author of the book being reviewed.

booktitle: title of the book being reviewed.

brdpage: location of this review in the Book Review Digest

price: price of the book, as reported in the BRD

publisher: this field also often includes the page count of the book, like "325p"

publication: reviewing publication. The previous five fields will be the same for all reviews of the same book. But "publication" will differ from one row to the next, because each row corresponds to a separate review. Sometimes this field will say "summary," which is a brief plot summary of the book, not always attributed to a specific publication.

citation: this is mostly information about the date, page numbers, etc of the review. Most of that I don't care about. The one thing I really do care about here is the word count of the review, which is ideally something like "180w," but can sometimes also be reported as "IOOw" through OCR errors. I'm going to clean up those errors outside the capsule.

quote: These are the words in the review excerpt, scrambled by being put in alphabetical order--which is equivalent to reducing them to wordcounts.

avgsentiment: The average sentiment value of reviews for the book, if reviews lacking sentiment codes are not included. We translate the plusses and minuses to a number by treating - as 1, -+ as 2, +- as 3, and + as 4.

avgsentwmissing: The average sentiment value of reviews for the book, if reviews lacking sentiment codes are replaced with the average value (for all reviews in this year).

numreviewswithsent: How many reviews had sentiment codes.

numreviewsofbk: The total number of reviews for this book.

The last four columns make it possible to adjust the sentiment value by treating missing data in a variety of ways. But they don't allow me to assign sentiment codes to individual reviews, except in the (rather uncommon) case where a book had only one review.

authtitlefromindex: In order to distinguish fiction from nonfiction, I've tried to fuzzy match authors and titles to the "fiction" section of the index. When I can do that inside the capsule, this field contains the author's last name, initials, and the book title as reported in the index. When I can't, the field contains "nonfiction." But that could be wrong. I'm going to confirm the fiction subset more reliably outside the capsule, by matching authors/titles to a manually scanned version of the index. The manual scanning will also allow me to enrich the data with genre/subject headings, but I will clearly indicate that those weren't extracted from the capsule. Sometimes (rarely) this column contains "DISCARD:" before the author-title; this indicates that it's a less reliable match produced from index rows that couldn't be parsed well.

matchcloseness: A measure of the closeness of the fuzzy match between author/title in the book text and in the index; basically, this is how confident I am that the book is fiction. In cases where I couldn't find a match, it will contain "0."
